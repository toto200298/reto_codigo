{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e96d37ef",
   "metadata": {},
   "source": [
    "# Coding Challenge\n",
    "\n",
    "### Submitted by  \n",
    "**Alfonso Cervantes Barragán**  \n",
    "*Data Scientist*  \n",
    "a.cervantes2012@hotmail.com\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df62024d",
   "metadata": {},
   "source": [
    "## Preprocesamiento y modelamiento enviado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "214a62fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Performance\n",
      "==================================================\n",
      "Accuracy: 0.7475\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86      1495\n",
      "           1       0.00      0.00      0.00       505\n",
      "\n",
      "    accuracy                           0.75      2000\n",
      "   macro avg       0.37      0.50      0.43      2000\n",
      "weighted avg       0.56      0.75      0.64      2000\n",
      "\n",
      "\n",
      "==================================================\n",
      "YOUR TASK: Evaluate thoroughly and improve this baseline\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Baseline Model - Simple Logistic Regression\n",
    "Use this as your starting point\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# I’m keeping this to reduce noise for the exercise,\n",
    "# but I personally prefer leaving warnings on and fixing the root cause.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(\"ecommerce_returns_train.csv\")\n",
    "test = pd.read_csv(\"ecommerce_returns_test.csv\")\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"Simple preprocessing pipeline\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # Encode categorical: product_category\n",
    "    le_category = LabelEncoder()\n",
    "\n",
    "    # I suggest fixing leakage here.\n",
    "    # Right now, if you call preprocess(test), you're fitting the encoder on test too.\n",
    "    # My recommendation: fit the encoder once on TRAIN and then reuse it to transform TEST.\n",
    "    df_processed[\"product_category_encoded\"] = le_category.fit_transform(\n",
    "        df_processed[\"product_category\"]\n",
    "    )\n",
    "\n",
    "    # Handle missing sizes (Fashion items only have sizes)\n",
    "    if df_processed[\"size_purchased\"].notna().any():\n",
    "        most_common_size = df_processed[\"size_purchased\"].mode()[0]\n",
    "\n",
    "        # I recommend avoiding inplace=True.\n",
    "        # It can create subtle side effects and pandas is moving away from it.\n",
    "        # Cleaner option: df_processed[\"size_purchased\"] = df_processed[\"size_purchased\"].fillna(most_common_size)\n",
    "        df_processed[\"size_purchased\"].fillna(most_common_size, inplace=True)\n",
    "\n",
    "        le_size = LabelEncoder()\n",
    "\n",
    "        # Same recommendation as category: fit on TRAIN only, transform on TEST with the same encoder.\n",
    "        # I’d also handle “unknown” sizes that appear only in test (otherwise production will break).\n",
    "        df_processed[\"size_encoded\"] = le_size.fit_transform(\n",
    "            df_processed[\"size_purchased\"]\n",
    "        )\n",
    "    else:\n",
    "        # I suggest always creating the column to avoid crashing later when selecting features.\n",
    "        df_processed[\"size_encoded\"] = 0\n",
    "\n",
    "    # Feature selection\n",
    "    feature_cols = [\n",
    "        \"customer_age\", \"customer_tenure_days\", \"product_category_encoded\",\n",
    "        \"product_price\", \"days_since_last_purchase\", \"previous_returns\",\n",
    "        \"product_rating\", \"size_encoded\", \"discount_applied\"\n",
    "    ]\n",
    "\n",
    "    # I recommend a quick NaN check here (and then impute/log).\n",
    "    # Missing values can quietly distort training and evaluation.\n",
    "    X = df_processed[feature_cols]\n",
    "    y = df_processed[\"is_return\"]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "X_train, y_train = preprocess(train)\n",
    "X_test, y_test = preprocess(test)\n",
    "\n",
    "# I’d change this design: preprocess() creates new encoders every time,\n",
    "# so train/test can end up with different mappings.\n",
    "# My recommendation: fit encoders once on train, reuse them everywhere (test + production), and persist them.\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# I suggest saving feature_cols too.\n",
    "# Column order mismatches are a very common production bug.\n",
    "\n",
    "# Train baseline model\n",
    "baseline_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# If returns are the minority class, I recommend trying class_weight=\"balanced\".\n",
    "# It's a small change that usually improves recall for the return class in a measurable way.\n",
    "baseline_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = baseline_model.predict(X_test_scaled)\n",
    "\n",
    "# I strongly recommend storing predict_proba too and tuning thresholds (0.3/0.4/0.5).\n",
    "# That's the simplest way to control precision vs recall without changing the model.\n",
    "\n",
    "# Basic evaluation\n",
    "print(\"Baseline Model Performance\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# I wouldn’t stop at accuracy here.\n",
    "# I recommend adding confusion matrix + recall/F1 for class 1 (returns), since accuracy can look fine while missing returns.\n",
    "\n",
    "# Save artifacts\n",
    "joblib.dump(baseline_model, \"baseline_model.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "# I recommend saving the encoders (and feature_cols) as artifacts too.\n",
    "# Otherwise you won't be able to reproduce the same preprocessing in production.\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"YOUR TASK: Evaluate thoroughly and improve this baseline\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308274f",
   "metadata": {},
   "source": [
    "**Observations – Alfonso –**\n",
    "The model looks “good” based on accuracy (0.7475), but in practice it is useless for detecting returns: for class 1 it has precision = 0.00, recall = 0.00, and F1 = 0.00, meaning it predicts almost everything as “no return.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12671f50",
   "metadata": {},
   "source": [
    "## Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f160ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Load test data\n",
    "test = pd.read_csv(\"ecommerce_returns_test.csv\")\n",
    "\n",
    "# Load artifacts\n",
    "model = joblib.load(\"baseline_model.pkl\")\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "\n",
    "# Reuse preprocessing (as in baseline)\n",
    "X_test, y_test = preprocess(test)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463e19aa",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11817096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7475\n",
      "Precision: 0.0000\n",
      "Recall   : 0.0000\n",
      "F1-score : 0.0000\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f320b",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fc89f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1495    0]\n",
      " [ 505    0]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47409a10",
   "metadata": {},
   "source": [
    "## Performance by Product Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ad901cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Electronics</th>\n",
       "      <td>0.828666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>607.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fashion</th>\n",
       "      <td>0.686594</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Home_Decor</th>\n",
       "      <td>0.809689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>289.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  accuracy  recall  support\n",
       "product_category                           \n",
       "Electronics       0.828666     0.0    607.0\n",
       "Fashion           0.686594     0.0   1104.0\n",
       "Home_Decor        0.809689     0.0    289.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an evaluation copy of the test data\n",
    "test_eval = test.copy()\n",
    "test_eval[\"y_true\"] = y_test.values\n",
    "test_eval[\"y_pred\"] = y_pred\n",
    "\n",
    "# Review model performance by product category\n",
    "performance_by_category = (\n",
    "    test_eval\n",
    "    .groupby(\"product_category\")\n",
    "    .apply(lambda df: pd.Series({\n",
    "        \"accuracy\": accuracy_score(df[\"y_true\"], df[\"y_pred\"]),\n",
    "        \"recall\": recall_score(df[\"y_true\"], df[\"y_pred\"]),\n",
    "        \"support\": len(df)\n",
    "    }))\n",
    "    .sort_values(\"recall\")\n",
    ")\n",
    "\n",
    "# Inspect categories where recall is weakest\n",
    "performance_by_category.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ea6e6",
   "metadata": {},
   "source": [
    "**Observation – Alfonso:**\n",
    "From my perspective, the model shows acceptable accuracy by category, but recall is zero across all of them. This tells me that it is not detecting returns in any product group and is mainly doing well by predicting the majority class. I want to be very clear about this limitation.\n",
    "\n",
    "## Key Questions – Part 1\n",
    "\n",
    "**Strengths and weaknesses**\n",
    "I see this model as a solid baseline in terms of simplicity: it is fast, easy to interpret, and straightforward to run. However, its biggest weakness is clear to me—it **does not detect returns**, and it almost always defaults to predicting “no return.”\n",
    "\n",
    "**Where does it fail most?**\n",
    "In my evaluation, it fails most when identifying orders that actually end up being returned, particularly in high-variability categories like Fashion, where patterns are harder to capture with a simple linear model.\n",
    "\n",
    "**Is accuracy the right metric? Why/why not?**\n",
    "In my opinion, accuracy is not the right metric on its own here. With imbalanced data, accuracy can look acceptable even when the model completely misses the class we care about. That’s why I focus more on recall and F1, which expose the real performance issues more honestly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf8dc3",
   "metadata": {},
   "source": [
    "## Part 2 – Business-Aligned Metrics\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. Define **“success”** in clear business terms.  \n",
    "2. Recommend **2–3 metrics** aligned with business goals.  \n",
    "3. Analyze **false positive vs. false negative** trade-offs.  \n",
    "4. Calculate the **financial impact** of model predictions.  \n",
    "5. Determine the **optimal decision threshold**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99565efa",
   "metadata": {},
   "source": [
    "In this problem, I do not define success as achieving high accuracy, but as **reducing the overall cost of product returns**. A return costs **\\$18**, while a preventive intervention costs **\\$3** and reduces the probability of a return by **35%**. Given this imbalance, missing a real return is significantly more expensive than intervening unnecessarily, so the model should be evaluated based on financial impact rather than purely technical metrics.\n",
    "\n",
    "The cost structure makes the trade-off very clear. Correctly identifying a return and acting costs \\$3 and can avoid losing \\$18, which is a strong positive outcome. An unnecessary intervention costs only \\$3 and is acceptable. However, failing to detect a real return results in losing the full \\$18. This means false negatives are far more damaging than false positives.\n",
    "\n",
    "Because of this, **accuracy alone is not an appropriate metric**. In the baseline, accuracy was around **75\\%**, yet the model failed to detect any returns at all. This shows that accuracy can look acceptable even when the model provides no real business value. Instead, I focus primarily on **recall for the return class**, while using precision to keep intervention costs under control.\n",
    "\n",
    "I also do not treat the default probability threshold of 0.5 as optimal. I would explicitly test lower thresholds (e.g., 0.3 or 0.4) and select the one that minimizes total expected cost. A model would be considered ready for a pilot deployment if it clearly reduces expected losses compared to doing nothing, improves recall for returns, and keeps intervention volume within reasonable limits. Even a simple baseline model can deliver meaningful business value under these criteria.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307bdafc",
   "metadata": {},
   "source": [
    "## Part 3: Model Improvement (20 minutes) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9ded58",
   "metadata": {},
   "source": [
    "**Hypothesis 1 – Class imbalance**\n",
    "Returns are typically a minority class. Using `class_weight=\"balanced\"` should improve recall and F1 for returns without changing the algorithm.\n",
    "\n",
    "**Hypothesis 2 – Regularization tuning**\n",
    "Light tuning of the regularization strength (`C`) can reduce under/over-regularization and improve generalization.\n",
    "\n",
    "**Methodological improvement**\n",
    "Fixed data leakage by fitting encoders and scaler **only on training data** and reusing them for test and production.\n",
    "\n",
    "\n",
    "**Validation & Overfitting Check**\n",
    "\n",
    "* Preprocessing fitted **only on TRAIN** → no leakage.\n",
    "* Hyperparameters selected via **5-fold CV**.\n",
    "* Train vs Test F1 are comparable → no strong overfitting signal.\n",
    "* Same preprocessing object persisted for production reproducibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df7ca270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Model Performance\n",
      "==================================================\n",
      "Best params: {'C': 0.1}\n",
      "\n",
      "TRAIN metrics\n",
      "Accuracy: 0.5503\n",
      "F1 (returns): 0.3688\n",
      "\n",
      "TEST metrics\n",
      "Accuracy: 0.5465\n",
      "F1 (returns): 0.3644\n",
      "\n",
      "Classification Report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.56      0.65      1495\n",
      "           1       0.28      0.51      0.36       505\n",
      "\n",
      "    accuracy                           0.55      2000\n",
      "   macro avg       0.53      0.54      0.51      2000\n",
      "weighted avg       0.65      0.55      0.58      2000\n",
      "\n",
      "\n",
      "Models saved for production reproducibility\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Improved Baseline Model - Logistic Regression (No Classes)\n",
    "\n",
    "What I changed vs. the original baseline:\n",
    "- I fit encoders and the scaler on TRAIN only (to avoid leakage).\n",
    "- I handled class imbalance to stop the model from ignoring returns.\n",
    "- I tuned regularization lightly (small grid on C) to improve generalization.\n",
    "- I compare TRAIN vs TEST metrics to sanity-check overfitting.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load data\n",
    "# ------------------------------------------------------------\n",
    "train = pd.read_csv(\"ecommerce_returns_train.csv\")\n",
    "test = pd.read_csv(\"ecommerce_returns_test.csv\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Encode categorical variables (fit on TRAIN only)\n",
    "# ------------------------------------------------------------\n",
    "le_category = LabelEncoder()\n",
    "le_size = LabelEncoder()\n",
    "\n",
    "train = train.copy()\n",
    "test = test.copy()\n",
    "\n",
    "# Fit on train, reuse on test (prevents leakage / mapping mismatch)\n",
    "train[\"product_category_encoded\"] = le_category.fit_transform(train[\"product_category\"])\n",
    "test[\"product_category_encoded\"] = le_category.transform(test[\"product_category\"])\n",
    "\n",
    "# Handle size feature consistently\n",
    "# I fit size encoder on TRAIN and map unseen sizes in TEST to a safe fallback\n",
    "if train[\"size_purchased\"].notna().any():\n",
    "    most_common_size = train[\"size_purchased\"].mode()[0]\n",
    "\n",
    "    train[\"size_purchased\"] = train[\"size_purchased\"].fillna(most_common_size)\n",
    "    test[\"size_purchased\"] = test[\"size_purchased\"].fillna(most_common_size)\n",
    "\n",
    "    train[\"size_encoded\"] = le_size.fit_transform(train[\"size_purchased\"])\n",
    "\n",
    "    # Keep production behavior stable: unseen sizes fall back to the most common train size\n",
    "    test_sizes = test[\"size_purchased\"].map(lambda x: x if x in le_size.classes_ else most_common_size)\n",
    "    test[\"size_encoded\"] = le_size.transform(test_sizes)\n",
    "else:\n",
    "    # Always create the column so feature selection never breaks\n",
    "    train[\"size_encoded\"] = 0\n",
    "    test[\"size_encoded\"] = 0\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Feature selection\n",
    "# ------------------------------------------------------------\n",
    "feature_cols = [\n",
    "    \"customer_age\", \"customer_tenure_days\", \"product_category_encoded\",\n",
    "    \"product_price\", \"days_since_last_purchase\", \"previous_returns\",\n",
    "    \"product_rating\", \"size_encoded\", \"discount_applied\"\n",
    "]\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "y_train = train[\"is_return\"]\n",
    "\n",
    "X_test = test[feature_cols]\n",
    "y_test = test[\"is_return\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Scaling (fit on TRAIN only)\n",
    "# ------------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Model + light tuning\n",
    "# ------------------------------------------------------------\n",
    "# My assumption: the returns class is minority, so I balance it to improve recall/F1.\n",
    "param_grid = {\"C\": [0.1, 1.0, 3.0]}\n",
    "\n",
    "base_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=2000,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    base_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1\",\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Evaluation (quick generalization check)\n",
    "# ------------------------------------------------------------\n",
    "train_pred = best_model.predict(X_train_scaled)\n",
    "test_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Improved Model Performance\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Best params: {grid.best_params_}\")\n",
    "\n",
    "print(\"\\nTRAIN metrics\")\n",
    "print(f\"Accuracy: {accuracy_score(y_train, train_pred):.4f}\")\n",
    "print(f\"F1 (returns): {f1_score(y_train, train_pred):.4f}\")\n",
    "\n",
    "print(\"\\nTEST metrics\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, test_pred):.4f}\")\n",
    "print(f\"F1 (returns): {f1_score(y_test, test_pred):.4f}\")\n",
    "print(\"\\nClassification Report (TEST):\")\n",
    "print(classification_report(y_test, test_pred))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save artifacts (so inference uses the exact same preprocessing)\n",
    "# ------------------------------------------------------------\n",
    "joblib.dump(best_model, \"model.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "joblib.dump(le_category, \"le_category.pkl\")\n",
    "joblib.dump(le_size, \"le_size.pkl\")\n",
    "joblib.dump(feature_cols, \"feature_cols.pkl\")\n",
    "\n",
    "print(\"\\nModels saved for production reproducibility\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f652928",
   "metadata": {},
   "source": [
    "**Observations:** When I look at these results, what stands out to me is how consistent the model behaves between training and test data. The numbers are almost the same, which gives me confidence that it’s not overfitting and that the validation setup is solid. The fact that the model settled on a low C value tells me the data doesn’t contain a very strong signal, so being conservative is the right call here. I also see the expected trade-off from balancing the classes: the model catches more returns, but it does so by raising more false alarms. To me, the accuracy staying flat isn’t a failure; it simply reflects the limits of the current features. The real improvement is that the model is now making clearer, more intentional decisions that would be easier to reason about and deploy in a real business setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85057c62",
   "metadata": {},
   "source": [
    "## Part 4: Deployment Planning (10 minutes) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e47a558",
   "metadata": {},
   "source": [
    "## Task 1: Production Monitoring Plan "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b1335d",
   "metadata": {},
   "source": [
    "### Metrics to Track\n",
    "In production, I would not focus on accuracy. What I care about is how the model behaves on returns, since that’s where the business impact is. I would track recall and F1-score for the return class to make sure we are still catching a meaningful portion of returns. I would also monitor precision, because too many false positives can quickly become an operational problem. On top of model metrics, I would continuously track data quality indicators such as missing values, unseen categories, and basic shifts in feature distributions.\n",
    "\n",
    "### Detecting Model Degradation\n",
    "To me, degradation is rarely a single bad number. I would look for consistent patterns: a sustained drop in return recall or F1 compared to the validation baseline, or sudden changes in the percentage of orders predicted as returns. Even if accuracy stays flat, these signals usually indicate that customer behavior or product mix has changed. I would complement this with simple distribution checks on key features to confirm whether the input data is drifting.\n",
    "\n",
    "### Alerts to Configure\n",
    "I would configure alerts that are informative rather than reactive. For example, if return recall drops by more than 10–15% for several consecutive periods, that should trigger an investigation. I would also alert on sharp week-over-week changes in the predicted return rate and on spikes in unseen categories or missing values. These alerts are meant to prompt analysis, not automatic retraining.\n",
    "\n",
    "### Retraining Strategy\n",
    "I see retraining as a controlled decision, not an automatic reaction. Under stable conditions, I would retrain on a regular cadence, such as quarterly. If monitoring shows persistent degradation that cannot be explained by data quality issues, I would trigger an earlier retraining cycle. In all cases, retraining would reuse the same preprocessing pipeline and be validated against a recent holdout set.\n",
    "\n",
    "### Rollback Criteria\n",
    "A rollback should be simple and fast. I would roll back if a newly deployed model performs worse than the current production model on live or shadow metrics, or if it creates excessive false positives that disrupt operations. In those cases, reverting to the last stable model is safer than trying to tune a degraded one in production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e4cab",
   "metadata": {},
   "source": [
    "## Task 2 : Stakeholder Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5833577f",
   "metadata": {},
   "source": [
    "This model is designed to identify orders that are likely to be returned, with the goal of helping the business act earlier and reduce avoidable return costs. From my perspective, the model’s value is not in maximizing accuracy, but in improving our ability to flag a meaningful portion of returns while keeping operational impact under control. In its current form, the model detects roughly half of all returns, which already provides actionable signal compared to a purely reactive process.\n",
    "\n",
    "In business terms, this means the model can be used to prioritize manual reviews, adjust fulfillment strategies, or trigger targeted interventions for high-risk orders. The expected return on investment comes from reducing handling, logistics, and customer service costs associated with returns. Even a modest reduction in unnecessary returns can generate positive ROI, given that the cost of deploying and maintaining the model is relatively low.\n",
    "\n",
    "There are, however, clear risks to consider. Customer behavior, product mix, and promotional strategies change over time, and any of these can degrade model performance. To mitigate this, I recommend continuous monitoring of return-related metrics and data quality indicators, along with a controlled retraining strategy. The model has been built with reproducibility in mind, which allows us to retrain or roll back safely when needed.\n",
    "\n",
    "Success after launch should be measured using business-facing metrics rather than technical ones alone. Key indicators include the percentage of returns successfully flagged, the operational cost of false positives, and the overall impact on return-related expenses. I would also recommend running a controlled A/B test, where a subset of orders uses model-driven actions while the rest follows the current process, to quantify incremental business value.\n",
    "\n",
    "Overall, I see this model as a solid first deployment: conservative, explainable, and easy to monitor. It is not a final solution, but a practical and low-risk step toward more data-driven decision-making around returns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a30429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854a2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7b0da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ab93e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926fadaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a9728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82badfda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c78392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982148ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
