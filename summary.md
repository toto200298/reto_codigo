# Executive Summary

This project focused on evaluating and improving a baseline model for predicting product returns in an e-commerce setting. I started from a simple logistic regression that achieved high accuracy but failed to identify any returns at all. From a business standpoint, this baseline was misleading: it looked strong on paper, yet provided no actionable value because it ignored the minority class that actually matters. The goal of the exercise was not to maximize a single metric, but to build a model that makes sensible, explainable trade-offs and can realistically be deployed and monitored in production.

My approach was intentionally incremental. First, I validated the baseline to understand its failure modes, confirming that it was effectively predicting “no return” for almost every order. I then focused on fixing methodological issues, especially data leakage in preprocessing, and on improving the model’s ability to detect returns. I introduced class imbalance handling, light regularization tuning, and proper train–test separation, while keeping the algorithm simple and interpretable. The emphasis was on rigor and reproducibility rather than complexity.

The key finding is that accuracy alone is not a meaningful metric for this problem. The baseline model reached roughly 75% accuracy but achieved zero recall on returns, making it unusable in practice. The improved model reduced overall accuracy to around 55%, but it now identifies approximately half of all returns, with stable performance between training and test sets. This trade-off is intentional and controlled: the model sacrifices raw accuracy to gain business-relevant signal, while showing no signs of overfitting.

In business terms, the improved model enables early identification of high-risk orders. This opens the door to targeted interventions such as manual review, fulfillment adjustments, or proactive customer communication. Even with moderate precision, catching a meaningful share of returns can reduce logistics, handling, and support costs. Given the low cost of deployment and maintenance, I expect positive ROI as long as model-driven actions are focused on high-impact segments rather than applied blindly.

From a deployment perspective, I recommend moving forward with this improved model as a first production version. It is conservative, explainable, and easy to monitor. Clear monitoring metrics, controlled retraining, and a simple rollback strategy are essential to manage behavioral and seasonal drift. I would also recommend an A/B test after launch to quantify incremental business value and guide future iterations. This model is not the final solution, but it is a solid and defensible starting point.
